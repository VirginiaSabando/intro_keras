{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<img src=\"res/itm_logo.jpg\" width=\"300px\">\n\n## Inteligencia Artificial - IAI84\n### Instituto Tecnológico Metropolitano\n#### Pedro Atencio Ortiz - 2019\n\n\nEn este notebook se aborda el tema de aprendizaje de máquina para clasificación binaria utilizando Regresión Logística:\n1. Propagación hacia adelante (forward propagation)\n2. Función de pérdida\n3. Función de costo\n4. Descenso del gradiente\n5. Predicción"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<hr>\n# 1. Operaciones básicas\n## 1.1. Propagación hacia adelante (backward propagation)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def linear_activation(W, b, X):\n    z = np.dot(W.T,X) + b\n    \n    return z",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def sigmoid(z):\n    '''\n    Returns sigmoid activation for array z\n    '''\n    a = 1. / (1. + np.exp(-z)) \n    \n    return a ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "seed = 2\nnp.random.seed(2)\nz = np.random.randn(1,3)\nprint(sigmoid(z))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "X = np.array([[1,2,3],[4,5,6]]).T\nprint(\"X: \",X)\n\nY = np.array([[0, 1]])\nprint(\"Y: \", Y)\n\nW = np.array([[0.4], [-0.5], [0.01]])\nprint(\"W: \", W)\n\nb = 0.3\nprint(\"b: \", b)\n\nA = sigmoid(linear_activation(W, b, X))\n\nprint(\"forward propagation: \", A)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<hr>\n## 1.2. Función de perdida"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def loss(y, a):\n    return -(y * np.log(a) + (1-y) * np.log(1-a))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "seed = 2 #to be able to verify your result\nnp.random.seed(seed)\nW = np.random.randn(2,1)\nb = np.random.rand()\nX = np.random.randn(2, 3)\n\nY = np.array([[1,1,0]]) #original labels for features X\nA = sigmoid(linear_activation(W,b,X)) #forward activation\n\nprint(\"Perdida dato a dato: \", loss(Y, A))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<hr>\n## 1.3. Función de costo"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def cost(logloss):\n    return np.mean(logloss)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "logloss = np.array([[0.22068428,  0.24198147,  1.27491702]])\nprint(\"costo: \", cost(logloss))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<hr>\n## 1.4. Descenso del gradiente (Gradient Descent) "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "seed = 2\nnp.random.seed(seed)\n\nX = np.random.rand(3,2)\nY = np.array([[0, 1]])\n\nm = X.shape[1]\n\nW = np.array([[0.1], [-0.1], [0.01]])\nb = 0.1\n\nprint(\"m: \", m)\nprint(\"W inicial: \",W)\nprint(\"b inicial: \",b)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "learning_rate = 0.1\n\nfor i in range(10000): #1000 iteraciones del descenso del gradiente\n    '''\n    Activacion hacia adelante\n    '''\n    Z = linear_activation(W,b,X)\n    A = sigmoid(Z)\n    \n    '''\n    Activacion hacia atras - retropropagacion del error\n    '''\n    dz = A - Y\n    dW = np.dot(X,dz.T) / m\n    db = np.sum(dz) / m\n    \n    '''\n    Actualizacion de parametros mediante descenso del gradiente\n    '''\n    W -= learning_rate * dW\n    b -= learning_rate * db\n    \n    J = cost(loss(Y,A))\n\n    if(i%1000 == 0):\n        print(\"costo: \", J)\n\nprint(\"W actualizado: \",W)\nprint(\"b actualizado: \",b)\nprint(\"costo total: \", J)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 1.5. Predicción\n\nLa predicción consiste en aplicar forward propagation utilizando los W y b optimizados mediante descenso del gradiente."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def predict(W,b,X):\n    z = linear_activation(W,b,X)\n    A = sigmoid(z)\n    return np.round(A)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "Y_hat = predict(W,b,X)\nprint(\"predicciones: \",np.round(Y_hat))\nprint(\"clases originales: \", Y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<hr>\n# 2. Regresión Logística sobre un dataset"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "'''\nUtility functions\n'''\n\nimport numpy as np\nimport sklearn\nfrom sklearn import datasets\nimport matplotlib.pyplot as plt\n\ndef generate_data(data_type, noise=0.2):\n    \"\"\"\n    Generate a binary dataset with distribution data_type\n\n    Arguments:\n    data_type -- distribution of dataset {moons,circles,blobs}\n\n    Returns:\n    X -- features\n    Y -- labels\n    \"\"\" \n    np.random.seed(0)\n    if data_type == 'moons':\n        X, Y = datasets.make_moons(200, noise=noise)\n    elif data_type == 'circles':\n        X, Y = sklearn.datasets.make_circles(200, noise=noise)\n    elif data_type == 'blobs':\n        X, Y = sklearn.datasets.make_blobs(centers=2, cluster_std=noise)\n    return X, Y\n\ndef visualize_lr(W, b, X, y):\n    \"\"\"\n    Plots a classification boundary for a logistic regression model\n    defined by W and b, using X (inputs) and y (outputs)\n\n    Arguments:\n    data_type -- distribution of dataset {moons,circles,blobs}\n\n    Returns:\n    W -- weights of lr model\n    b -- bias of lr model\n    X -- features\n    y -- labels\n    \"\"\"\n    X = X.T\n    # Set min and max values and give it some padding\n    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n    h = 0.01\n    # Generate a grid of points with distance h between them\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    # Predict the function value for the whole gid\n    Z = predict(W,b,np.c_[xx.ravel(), yy.ravel()].T)\n    Z = Z.reshape(xx.shape)\n    # Plot the contour and training examples\n    plt.figure(figsize=(7,5))\n    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n    \n    color= ['black' if y == 1 else 'yellow' for y in np.squeeze(Y)]\n    plt.scatter(X[:, 0], X[:, 1], cmap=plt.cm.Spectral, c=color)\n    plt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<hr>\n## 2.1. Generación del dataset"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "X, Y = generate_data('blobs', 1.5)\nY = Y.reshape(1,len(Y))\nprint(X.shape)\nprint(Y.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "color= ['black' if y == 1 else 'yellow' for y in np.squeeze(Y)]\n\nplt.figure(figsize=(7,5))\nplt.scatter(X[:,0], X[:,1], color=color)\n\nplt.show()\n\nX = X.T",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<hr>\n## 2.2. Inicialización de parámetros del regresor"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#1. inicilicemos parametros W y b\nseed = 3\nnp.random.seed(seed)\n\nm = X.shape[1]\n\nW = np.random.randn(X.shape[0], 1)\nb = np.random.random()\n\nprint(\"m: \", m)\nprint(\"W inicial: \",W)\nprint(\"b inicial: \",b)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "'''\nPrimero visualicemos cual seria la clasificacion con valores de W y b aleatorios.\n'''\nvisualize_lr(W, b, X, Y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<hr>\n## 2.3. Entrenamiento del regresor mediante descenso del gradiente"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#2. Regresion logistica mediante descenso del gradiente\n\nlearning_rate = 0.1\n\nnum_epochs = 10000\n\nfor i in range(num_epochs): #1000 iteraciones del descenso del gradiente\n    '''\n    Activacion hacia adelante\n    '''\n    Z = linear_activation(W,b,X)\n    A = sigmoid(Z)\n    \n    '''\n    Activacion hacia atras - retropropagacion del error\n    '''\n    dz = A - Y\n    dW = np.dot(X,dz.T) / m\n    db = np.sum(dz) / m\n    \n    '''\n    Actualizacion de parametros mediante descenso del gradiente\n    '''\n    W -= learning_rate * dW\n    b -= learning_rate * db\n    \n    J = cost(loss(Y,A))\n\n    if(i%1000 == 0):\n        print(\"costo: \", J)\n\nprint(\"W actualizado: \",W)\nprint(\"b actualizado: \",b)\nprint(\"costo final (error), despues de \",i+1,\" iteraciones: \", J)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "visualize_lr(W, b, X, Y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.15",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 2,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
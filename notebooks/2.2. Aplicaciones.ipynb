{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img align=\"center\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/5/53/UNAL_Aplicación_Medell%C3%ADn.svg/1280px-UNAL_Aplicación_Medell%C3%ADn.svg.png\" width=\"300\"/></th>\n",
    "    <th><img align=\"center\" src=\"http://www.redttu.edu.co/es/wp-content/uploads/2016/01/itm.png\" width=\"300\"/> </th> \n",
    "    <th><img align=\"center\" src=\"https://www.cienciasdelaadministracion.uns.edu.ar/wp-content/themes/enlighten-pro/images/logo-uns-horizontal.png\" width=\"300\"/></th>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "#### Pedro Atencio Ortiz - 2019 (pedroatencio@itm.edu.co)\n",
    "\n",
    "# Módulo 2.2. Aplicaciones\n",
    "\n",
    "En este notebook abordaremos los siguientes tópicos:\n",
    "\n",
    "1. Clasificación y Regresión.\n",
    "2. Redes convolutivas.\n",
    "3. Redes recurrentes\n",
    "4. Utilizando una red profunda pre-entrenada.\n",
    "5. Deep features / Latent spaces.\n",
    "6. Fine-tuning: Utilizar una red pre-entrenada y afinarla para que trabaje con nuestros datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones utilitarias\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def generate_data(data_type, noise=0.2, num_samples=200):\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    if data_type == 'moons':\n",
    "        X, Y = datasets.make_moons(num_samples, noise=noise)\n",
    "    elif data_type == 'circles':\n",
    "        X, Y = sklearn.datasets.make_circles(num_samples, noise=noise)\n",
    "    elif data_type == 'blobs':\n",
    "        X, Y = sklearn.datasets.make_blobs(centers=2, cluster_std=noise)\n",
    "    return X, Y\n",
    "\n",
    "def visualize_model(model, X, Y, output='truncate'):\n",
    "    XT = np.copy(X)\n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = XT[:, 0].min() - .5, XT[:, 0].max() + .5\n",
    "    y_min, y_max = XT[:, 1].min() - .5, XT[:, 1].max() + .5\n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    # Predict the function value for the whole gid\n",
    "    if(output=='truncate'):\n",
    "        Z = np.round(model.predict(np.c_[xx.ravel(), yy.ravel()]))\n",
    "    elif(output=='same'):\n",
    "        Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    else:\n",
    "        print(\"output param must be either truncate or same\")\n",
    "        return False\n",
    "\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot the contour and training examples\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.bone)\n",
    "\n",
    "    color = ['blue' if y == 1 else 'red' for y in np.squeeze(Y)]\n",
    "    plt.scatter(X[:,0], X[:,1], color=color)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 1. Clasificación y Regresión.\n",
    "\n",
    "Las redes neuronales pueden ser configuradas para resolver problemas de clasificación (salida==etiqueta) o de regresión (salida==valor). A continuación analizaremos dos ejemplos, uno para cada caso y discutiremos las configuraciones necesarias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Clasificación\n",
    "\n",
    "Trabajemos sobre el dataset fashion-mnist tomando cada imagen como un array de valores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input, LeakyReLU, Activation\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "\n",
    "#Transformamos las etiquetas de salida a one-hot encoding.\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "print(\"Total de imagenes: \", len(x_train)+len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 76\n",
    "plt.title(\"Categoria: \"+str(y_train[i]))\n",
    "plt.imshow(x_train[i], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilicemos las capas tipo __Dense__ con activaciones __elu__ en las capas intermedias, y __softmax__ para la salida. También utilicemos la capa __Flatten__ para aplanar la imagen de entrada a un vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session() #borra el grafo de la sesion. Util cuando creamos muchos modelos en una sesion.\n",
    "\n",
    "# Red neuronal\n",
    "model = Sequential()\n",
    "model.add(Input((28,28))) \n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=512, use_bias=True))\n",
    "model.add(Activation(activation=LeakyReLU(alpha=0.2)))\n",
    "model.add(Dense(units=10, activation='softmax', use_bias=True))\n",
    "\n",
    "opt = RMSprop(lr=0.001)\n",
    "\n",
    "model.compile(optimizer=opt,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, epochs=10, batch_size=64, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Error (Loss) final: %.4E\"%(np.array(history.history['loss'][-1:]))) #Error final de la lista de errores\n",
    "print(\"Precision (Accuracy) final: %.4f\"%(np.array(history.history['acc'][-1:])))\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['acc'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluemos el modelo\n",
    "\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lancemos una prediccion con nuestro modelo entrenado\n",
    "\n",
    "i = 104\n",
    "\n",
    "pred = np.round(model.predict(x_test[i].reshape(1,28,28)))\n",
    "print(pred)\n",
    "print(y_test[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Trabajemos\n",
    "\n",
    "<br>\n",
    "\n",
    "<font size=4>\n",
    "\n",
    "1. Intente utilizar un batch_size menor y uno mayor, por ejemplo, 64 y 512. Qué puede observar?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Regresión\n",
    "\n",
    "Intentemos construir un modelo que permita predecir valores de una serie de tiempo a partir de un conjunto de datos en una ventana de tiempo.\n",
    "\n",
    "Generalmente, este tipo de problemas no tiene un dataset $(X, Y)$ asociado, sino una secuencia $X$, a partir de la cual se debe construir un dataset $(X[a,b], X[c])$, donde a,b y c son valores de tiempo y $a<b<c$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generemos una serie de tiempo sintetica con estacionalidad\n",
    "\n",
    "def plot_series(time, series, format=\"-\", start=0, end=None):\n",
    "    plt.plot(time[start:end], series[start:end], format)\n",
    "    plt.xlabel(\"Tiempo\")\n",
    "    plt.ylabel(\"Valor\")\n",
    "    plt.grid(False)\n",
    "\n",
    "def trend(time, slope=0):\n",
    "    return slope * time\n",
    "\n",
    "def seasonal_pattern(season_time):\n",
    "    \"\"\"Just an arbitrary pattern, you can change it if you wish\"\"\"\n",
    "    return np.where(season_time < 0.1,\n",
    "                    np.cos(season_time * 6 * np.pi),\n",
    "                    2 / np.exp(9 * season_time))\n",
    "\n",
    "def seasonality(time, period, amplitude=1, phase=0):\n",
    "    \"\"\"Repeats the same pattern at each period\"\"\"\n",
    "    season_time = ((time + phase) % period) / period\n",
    "    return amplitude * seasonal_pattern(season_time)\n",
    "\n",
    "def noise(time, noise_level=1, seed=None):\n",
    "    rnd = np.random.RandomState(seed)\n",
    "    return rnd.randn(len(time)) * noise_level\n",
    "\n",
    "time = np.arange(10 * 365 + 1, dtype=\"float32\")\n",
    "series = trend(time, 0.1)  \n",
    "baseline = 10\n",
    "amplitude = 40\n",
    "slope = 0.005\n",
    "noise_level = 3\n",
    "\n",
    "# Create the series\n",
    "series = baseline + trend(time, slope) + seasonality(time, period=365, amplitude=amplitude)\n",
    "\n",
    "# Update with noise\n",
    "series += noise(time, noise_level, seed=51)\n",
    "\n",
    "split_time = 3000\n",
    "time_train = time[:split_time]\n",
    "x_train = series[:split_time]\n",
    "time_valid = time[split_time:]\n",
    "x_valid = series[split_time:]\n",
    "\n",
    "plt.figure(figsize=(15,3))\n",
    "plot_series(time, series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(series)\n",
    "    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n",
    "    dataset = dataset.shuffle(shuffle_buffer).map(lambda window: (window[:-1], window[-1]))\n",
    "    dataset = dataset.batch(batch_size).prefetch(1)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 20\n",
    "batch_size = 32\n",
    "shuffle_buffer_size = 1000\n",
    "\n",
    "dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=256, input_dim=window_size, activation='elu'))\n",
    "model.add(Dense(units=1, activation=\"linear\"))\n",
    "\n",
    "model.compile(loss='mse', \n",
    "              optimizer=tf.keras.optimizers.SGD(lr=8e-6, momentum=0.9),\n",
    "             metrics=['mae'])\n",
    "history = model.fit(dataset,epochs=100,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = []\n",
    "for time in range(len(series) - window_size):\n",
    "    forecast.append(model.predict(series[time:time + window_size][np.newaxis]))\n",
    "\n",
    "forecast = forecast[split_time-window_size:]\n",
    "results = np.array(forecast)[:, 0, 0]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plot_series(time_valid, x_valid)\n",
    "plot_series(time_valid, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trabajemos\n",
    "\n",
    "1. Evalúe el modelo entrenado sobre el dataset de prueba __time_valid__ y imprima el loss y el mae."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 2. Redes convolutivas\n",
    "\n",
    "Una red convolutiva es una arquitectura compuesta por dos etapas: 1) etapa convolutiva, que se encarga de realizar el feature engineering y 2) etapa fully connected, que estima las categorías a partir de las características extraidas en la primera etapa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "#Etapa convolutiva\n",
    "convModel = Sequential()\n",
    "convModel.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu', input_shape=(32,32,3)))\n",
    "convModel.add(MaxPooling2D(2,2))\n",
    "convModel.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu'))\n",
    "convModel.add(MaxPooling2D(2,2))\n",
    "\n",
    "#Etapa fully connected\n",
    "convModel.add(Flatten())\n",
    "convModel.add(Dense(128, activation='relu'))\n",
    "convModel.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10 = tf.keras.datasets.cifar10\n",
    "(x_train, y_train),(x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "#Requerimiento de la capa convolutiva input_shape = (batch_size, m, n, c)\n",
    "x_train = x_train.reshape(50000, 32, 32, 3)\n",
    "x_test = x_test.reshape(10000, 32, 32, 3)\n",
    "\n",
    "#Transformamos las etiquetas de salida a one-hot encoding.\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convModel.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "convModel.fit(x_train, y_train, epochs=5)\n",
    "convModel.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 3. Redes recurrentes \n",
    "\n",
    "Esta arquitectura de red contienen capas con neuronas que permiten tomar como entrada, salidas de la misma capa en instantes anteriores. Por ello son muy útiles para analizar series de tiempo. Ejemplos de datos que pueden tratarse como series de tiempo son: __video__, __audio__ y __texto__.\n",
    "\n",
    "Trabajemos sobre el mismo ejemplo de series de tiempo que utilizamos en al sección 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM, Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 20\n",
    "batch_size = 32\n",
    "shuffle_buffer_size = 1000\n",
    "\n",
    "dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "#Etapa recurrente\n",
    "rnn_model = Sequential()\n",
    "\n",
    "#Requerimiento de la capa recurrente input_shape = (batch_size, window_size, dimension del dato)\n",
    "rnn_model.add(Lambda(lambda x: tf.expand_dims(x, axis=-1), input_shape=[None]))\n",
    "rnn_model.add(LSTM(units=32, return_sequences=True))\n",
    "rnn_model.add(LSTM(units=32))\n",
    "\n",
    "#Etapa fully connected\n",
    "rnn_model.add(Dense(1, activation='sigmoid'))\n",
    "rnn_model.add(Lambda(lambda x: x*100.0))\n",
    "\n",
    "opt = tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)\n",
    "\n",
    "rnn_model.compile(loss='mse',\n",
    "              optimizer=opt,\n",
    "              metrics=[\"mae\"])\n",
    "\n",
    "history = rnn_model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = []\n",
    "for time in range(len(series) - window_size):\n",
    "    forecast.append(rnn_model.predict(series[time:time + window_size][np.newaxis]))\n",
    "\n",
    "forecast = forecast[split_time-window_size:]\n",
    "results = np.array(forecast)[:, 0, 0]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plot_series(time_valid, x_valid)\n",
    "plot_series(time_valid, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 4. Utilizando una red profunda pre-entrenada\n",
    "\n",
    "Keras permite cargar modelos pre-entrenados de clasificación de imágenes, que podemos utilizar directamente en alguna aplicación.\n",
    "\n",
    "Analicemos el modelo VGG16 entrenado para el dataset __image-net__.\n",
    "\n",
    "<img align=\"center\" src=\"https://github.com/psatencio/intro_keras/blob/master/figures/vgg16.png?raw=true\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input, decode_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "base_model = VGG16(include_top=True, weights='imagenet', \n",
    "                   input_tensor=None, input_shape=None, \n",
    "                   pooling=None, classes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(base_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "cifar10 = tf.keras.datasets.cifar10\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = x_train[967]\n",
    "\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debido a que cada modelo pre-entrenado tiene sus propias especificaciones para las entradas, es necesario transformar nuestro conjunto de datos de acuerdo a dichas especificaciones. Para ello utilicemos las funciones de transformación propias para el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "\n",
    "def preprocess_image(img, size=(224,224)):\n",
    "\n",
    "    x = resize(img, output_shape=size)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = x.astype(np.float64)*255.0\n",
    "\n",
    "    x = vgg16.preprocess_input(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = preprocess_image(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lanzamos la prediccion y consultamos la categoria\n",
    "\n",
    "pred = base_model.predict(img)\n",
    "categoria = decode_predictions(pred, top=2) #mejores 2 activaciones\n",
    "\n",
    "print(categoria)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 4. Deep features / Latent spaces\n",
    "\n",
    "En deep learning es posible utilizar la activación de una capa intermedia de una red neuronal entrenada para un problema, y utilizar dicha activacion como caracteristicas de entrada para un nuevo clasificador utilizado en otro problema. Estas características reciben el nombre de __deep features__.\n",
    "\n",
    "Analicemos el modelo VGG16 entrenado para el dataset __image-net__.\n",
    "\n",
    "<img align=\"center\" src=\"https://github.com/psatencio/intro_keras/blob/master/figures/deep_features.png?raw=true\" width=\"500\"/>\n",
    "\n",
    "Algunas aplicaciones de las deep-features:\n",
    "\n",
    "- Representación vectorial de objetos.\n",
    "- Entrenamiento de nuevos modelos de clasificación o regresión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "#include_top=False se utiliza para cargar solo la parte convolutiva del modelo\n",
    "\n",
    "base_model = VGG16(include_top=False, weights='imagenet', \n",
    "                   input_tensor=None, input_shape=None, \n",
    "                   pooling=None, classes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "cifar10 = tf.keras.datasets.cifar10\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = x_train[967]\n",
    "\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-procesamos la imagen para que se adapte a los requerimiento de VGG16\n",
    "\n",
    "img = preprocess_image(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtengamos las deep-features para la imagen img\n",
    "\n",
    "deep_feat = base_model.predict(img)\n",
    "print(deep_feat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En algunos casos es posible necesitar obtener la salida de cualquier capa de la red y no necesariamente la ultima de la parte convolutiva. En este caso podemos construir un nuevo modelo haciendo referencias a las capas que necesitamos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cargamos el modelo completo\n",
    "base_model = VGG16(include_top=True, weights='imagenet', \n",
    "                   input_tensor=None, input_shape=None, \n",
    "                   pooling=None, classes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construccion del nuevo modelo a partir del modelo anterior\n",
    "#Nota: Utilizamos la forma de construccion C del notebook 2.1.\n",
    "    \n",
    "#x = model.layers[-3].output\n",
    "x = base_model.get_layer('fc1').output\n",
    "\n",
    "new_model = Model(inputs=base_model.input, outputs=x) #conexion del nuevo modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_feat = new_model.predict(img)\n",
    "print(deep_feat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 5. Fine-tuning\n",
    "\n",
    "Consiste en tomar un modelo pre-entrenado y utilizarlo para una nueva tarea. De manera general los pasos consisten en:\n",
    "\n",
    "1. Congelar los pesos de la red hasta la capa en la que deseamos los deep-features.\n",
    "2. Agregar nuevas capas al modelo que representen nuestro problema o tarea.\n",
    "3. Crear un nuevo modelo que incluya las capas del modelo pre-entrenado y las nuevas capas.\n",
    "4. Entrenar los pesos desde la conexión entre los deep-features y las nuevas capas.\n",
    "\n",
    "<img align=\"center\" src=\"https://github.com/psatencio/intro_keras/blob/master/figures/fine_tuning.png?raw=true\" width=\"500\"/>\n",
    "\n",
    "Apliquemos este concepto para el problema del dataset CIFAR-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "base_model = VGG16(include_top=False, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(base_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(base_model.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Congelar los pesos de la red hasta la capa en la que deseamos los deep-features.\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "#2. Agregar nuevas capas al modelo que representen nuestro problema o tarea.\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x) #toma los filtros de convolucion y promedia sus valores\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predicciones = Dense(10, activation='softmax', name='nueva_salida')(x) #10 categorias\n",
    "\n",
    "#3. Crear un nuevo modelo que incluya las capas del modelo pre-entrenado y las nuevas capas.\n",
    "new_model = Model(inputs=base_model.input, outputs=predicciones)\n",
    "\n",
    "new_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "cifar10 = tf.keras.datasets.cifar10\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = x_train[967]\n",
    "\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Entrenar los pesos desde la conexión entre los deep-features y las nuevas capas.\n",
    "history = new_model.fit(x_train[:10000], y_train[:10000], epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Trabajemos\n",
    "\n",
    "En algunos casos el flujo de trabajo del fine-tuning consiste en los siguientes pasos:\n",
    "\n",
    "1. Primer ajuste\n",
    "    1. Congelar los pesos de la red hasta la capa en la que deseamos los deep-features.\n",
    "    2. Agregar nuevas capas al modelo que representen nuestro problema o tarea.\n",
    "    3. Crear un nuevo modelo que incluya las capas del modelo pre-entrenado y las nuevas capas.\n",
    "    4. Entrenar en pocas épocas los pesos desde la conexión entre los deep-features y las nuevas capas.\n",
    "\n",
    "2. Segundo ajuste\n",
    "    1. Descongelar una o dos capas del modelo pre-entrenado, contando desde la conexión a las nuevas capas.\n",
    "    2. Entrenar complementamente la(s) capa(s) descongelas y las nuevas capas.\n",
    "\n",
    "Intentemos realizar esta implementación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img align=\"center\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/5/53/UNAL_Aplicación_Medell%C3%ADn.svg/1280px-UNAL_Aplicación_Medell%C3%ADn.svg.png\" width=\"300\"/></th>\n",
    "    <th><img align=\"center\" src=\"http://www.redttu.edu.co/es/wp-content/uploads/2016/01/itm.png\" width=\"300\"/> </th> \n",
    "    <th><img align=\"center\" src=\"https://www.cienciasdelaadministracion.uns.edu.ar/wp-content/themes/enlighten-pro/images/logo-uns-horizontal.png\" width=\"300\"/></th>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "#### Pedro Atencio Ortiz - 2019\n",
    "\n",
    "# 1. Regresor logístico como una neurona\n",
    "\n",
    "En este notebook abordaremos los siguientes tópicos:\n",
    "\n",
    "- Análisis desde el grafo de cómputo.\n",
    "- Implementación tradicional.\n",
    "- Broadcasting / Vectorization.\n",
    "- Implementación vectorizada.\n",
    "- Descenso del gradiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 1.1. Análisis desde el grafo de cómputo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un regresor logístico es un modelo de __clasificación lineal__ definido así: \n",
    "\n",
    "<center><font size=5>$a = f(wx+b)$</font></center>\n",
    "\n",
    "Donde $f$ es la función sigmoide definida como: \n",
    "\n",
    "<center><font size=5>$f(z) = sigmoid(z) = \\frac{1}{1 + e^{-z}}$</font></center>\n",
    "\n",
    "Finalmente, para poder optimizar este modelo, es necesario contar con una estimación de su error o pérdida. Para ello utilizamos la función __logistic loss__ o pérdida logística definida cómo:\n",
    "\n",
    "<br>\n",
    "\n",
    "<center><font size=5>$logloss(a, y) = l(a, y) = -(y.log(a) + (1-y).log(1-a))$</font></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "Una forma de analizar este proceso consiste en desplegarlo como un grafo de cómputo. En la siguiente gráfica se puede observar la secuencia de procesamiento desde la entrada $x$ hacia la salida $a$ de un __regresor logístico__, así como la función de error $loss$ o $l$ que depende de la salida $a$ y del valor esperado $y$.\n",
    "\n",
    "<img align=\"center\" src=\"https://github.com/psatencio/intro_keras/blob/master/figures/grafo_regresor_logistico.png?raw=true\" width=\"700\"/>\n",
    "\n",
    "Implementemos este flujo de datos de una manera simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1. / (1. + math.exp(-z))\n",
    "\n",
    "def logloss(a, y):\n",
    "    return -(y * math.log(a) + (1-y) * math.log(1-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizemos como se comportan la funcion sigmoide y el error logistico\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "x = np.linspace(-5,5,30)\n",
    "y = np.array([sigmoid(z) for z in x]) #comprehension\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.grid()\n",
    "plt.plot(x,y)\n",
    "plt.xlabel(\"z\", fontsize=12)\n",
    "plt.ylabel(r'$sigmoid(z)$', fontsize=12)\n",
    "\n",
    "plt.subplot(122)\n",
    "epsilon = 1E-5\n",
    "a = np.linspace(epsilon,1-epsilon, 100) #prediction\n",
    "l = np.array([logloss(t,(1-t)) for t in a]) #comprehension\n",
    "\n",
    "plt.grid()\n",
    "plt.plot(a,l)\n",
    "plt.xlabel(r'$a$', fontsize=12)\n",
    "plt.ylabel(r'$logloss(a, (1-a))$', fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 1.2. Implementación tradicional\n",
    "\n",
    "Ahora procedamos a implementar el grafo de cómputo del regresor logístico a partir de lo definido anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parametros del regresor\n",
    "w = -0.5\n",
    "b = -1.5\n",
    "\n",
    "#ejemplo del dataset\n",
    "x = -1.2\n",
    "y = 1\n",
    "\n",
    "z = w * x + b\n",
    "a = sigmoid(z) #prediccion\n",
    "l = logloss(a, y) #error logistico\n",
    "\n",
    "print(\"Prediccion: %.4f \"%(a))\n",
    "print(\"Error: %.4f\"%(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, la implementación anterior funciona para un solo ejemplo de un dataset, y por ello, en caso de querer computar todas las predicciones para un dataset, es necesario utilizar ciclos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "\n",
    "#dataset\n",
    "X = np.random.random([10]) #100 inputs aleatorios\n",
    "Y = np.round(np.random.random([10])) #100 clases aleatorias 0,1\n",
    "\n",
    "#parametros del regresor\n",
    "w = -0.5\n",
    "b = -1.5\n",
    "\n",
    "for (i,x) in enumerate(X):\n",
    "    a = sigmoid(w * x + b)\n",
    "    l = logloss(a, Y[i])\n",
    "    \n",
    "    print(\"prediccion - dato %i: %.4f\"%(i,a))\n",
    "    print(\"logloss - dato %i: %.4f\"%(i,l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "<font size=4>\n",
    "    \n",
    "__Analicemos__: \n",
    "\n",
    "En el ejemplo anterior, tanto la entrada como la salida tenian dimension $\\rm I\\!R$. Qué pasaría con la implementación anterior si la entrada tuviera dimensión $\\rm I\\!R^{2}?$\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 1.3. Vectorization / Broadcasting\n",
    "\n",
    "Algunas operaciones entre conjuntos de datos pueden ser vectorizadas y expandidas (broadcasted) en Python utilizando para ello algunas operaciones algebraicas y la librería Numpy. Este es el caso de la operacion $z = wx+b$ que para el caso en que $x \\in \\rm I\\!R^{nx}$ equivale a la sumatoria de cada caracteristica por sample , multiplicada por su respectivo $w$, lo que finalmente equivale a un __producto punto__ (escalar) entre $x$ y $w$:\n",
    "\n",
    "<br>\n",
    "\n",
    "<font size=5><center>$\\sum_j^{nx}{x_j*w_j} = w^T.x$</center></font>\n",
    "\n",
    "Esto, además de facilitar la implementación del regresor, hace más eficiente su cálculo. Discutamos el siguiente ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generemos 10 millones de datos y computemos la sumatoria de su productoria\n",
    "\n",
    "np.random.seed(2)\n",
    "n = int(10E6)\n",
    "A = np.random.rand(n)\n",
    "B = np.random.rand(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forma tradicional\n",
    "tic = time()\n",
    "\n",
    "s = 0\n",
    "for i in range(n):\n",
    "    s += A[i] * B[i]\n",
    "\n",
    "toc = time()\n",
    "\n",
    "print(\"Resultado: %.4f\"%(s))\n",
    "print(\"Tiempo transcurrido (ms)\", (toc-tic)*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forma vectorizada\n",
    "tic = time()\n",
    "\n",
    "s = np.dot(A, B)\n",
    "\n",
    "toc = time()\n",
    "\n",
    "print(\"Resultado: %.4f \"%(s))\n",
    "print(\"Tiempo transcurrido (ms)\", (toc-tic)*1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Ahora, debemos considerar que nuestro dataset tiene un tamaño $m$, es decir, $m$ ejemplos, cada uno de dimensión $\\rm I\\!R^{nx}$. __Implica esto que ahora debamos hacer un ciclo para iterar por todo el dataset?__\n",
    "\n",
    "La librería numpy soporta una operación conocida como __broadcast__, la cual permite replicar operaciones entre dos arreglos de distinta dimensión, siempre que estos compartan una de las dimensiones y uno de ellos tenga magnitud 1 en una de sus dimensiones. Para ello Python replicará uno de los dos vectores y ejecutará la operación. Esto se puede generalizar de la siguiente manera:\n",
    "\n",
    "<br>\n",
    "\n",
    "<center><font size=5>$A_{(m,n)} + B_{(m, 1)} \\rightarrow  A_{(m,n)} + B_{(m, n)}$</font></center>\n",
    "<center><font size=5>$A_{(m,n)} + B_{(1, n)} \\rightarrow  A_{(m,n)} + B_{(m, n)}$</font></center>\n",
    "\n",
    "\n",
    "Analicemos el siguiente ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "\n",
    "A = np.random.rand(4,3)\n",
    "k = np.random.rand(4,1)\n",
    "\n",
    "print(\"Array A: \\n\", A)\n",
    "print(\"Array B: \\n\", k)\n",
    "\n",
    "print(\"A + B:\\n \",A+k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 1.4. Implementación vectorizada\n",
    "\n",
    "A partir de lo anterior, entonces podemos concluir que el producto punto en Python vectoriza la operación $wX+b$ y a la vez extiende la operación para todos los datos del dataset por broadcast. Revisemos ahora la implementación vectorizada del regresor logístico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation(W, b, X):\n",
    "    z = np.dot(W.T,X) + b\n",
    "    \n",
    "    return z\n",
    "\n",
    "def sigmoid(z):\n",
    "    a = 1. / (1. + np.exp(-z)) \n",
    "    \n",
    "    return a \n",
    "\n",
    "def logloss(y, a):\n",
    "    return -(y * np.log(a) + (1-y) * np.log(1-a))\n",
    "\n",
    "def cost(L):\n",
    "    return np.mean(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "\n",
    "#dataset\n",
    "X = np.random.random([1, 10]) #100 inputs aleatorios\n",
    "Y = np.round(np.random.random([10])) #100 clases aleatorias 0,1\n",
    "\n",
    "#parametros del regresor\n",
    "w = np.array([-0.5])\n",
    "b = -1.5\n",
    "\n",
    "Z = linear_activation(w, b, X)\n",
    "A = sigmoid(Z)\n",
    "L = logloss(Y, A)\n",
    "    \n",
    "print(\"Predicciones: \",A,\"\\n\")\n",
    "print(\"Errores: \", L, \"\\n\")\n",
    "print(\"Error total (costo): \", cost(L))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 1.5. Descenso del gradiente\n",
    "\n",
    "El descenso del gradiente, es un algoritmo de optimización que utiliza el gradiente del error respecto a los parámetros de una función, para actualizar los mismos de manera iterativa hasta estabilizarse en un mínimo.\n",
    "\n",
    "<br>\n",
    "\n",
    "Sea $f(\\theta)$ una función __diferenciable__ para la cual se desea encontrar el valor de $\\theta$ tal que la minimice, el descenso del gradiente se puede plantear como:\n",
    "\n",
    "<br>\n",
    "\n",
    "<font size=3>\n",
    "$\n",
    "\\\\\n",
    "repetir\\{\n",
    "\\\\\n",
    "\\hspace{10mm} d \\theta = \\frac{df}{d \\theta}\n",
    "\\\\\n",
    "\\hspace{10mm} \\theta = \\theta - \\alpha d \\theta\n",
    "\\\\\n",
    "\\}\n",
    "$\n",
    "</font>\n",
    "\n",
    "\n",
    "En el caso concreto del regresor logístico, nos interesa encontrar los parametros $w$ y $b$ tales que minicen el error $logloss$. En ese caso el descenso del gradiente queda:\n",
    "\n",
    "<br>\n",
    "\n",
    "<font size=3>\n",
    "$\n",
    "\\\\\n",
    "repetir\\{\n",
    "\\\\\n",
    "\\hspace{10mm} dw = \\frac{dLoss}{dw}\n",
    "\\\\\n",
    "\\hspace{10mm} db = \\frac{dLoss}{db}\n",
    "\\\\\n",
    "\\hspace{10mm} w = w - \\alpha. dw\n",
    "\\\\\n",
    "\\hspace{10mm} b = b - \\alpha. db\n",
    "\\\\\n",
    "\\}\n",
    "$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "<font size=3>\n",
    "    \n",
    "__Trabajemos__:\n",
    "\n",
    "Desarrollemos las derivadas de los parámetros $w$ y $b$ a partir del gráfo de cómputo.\n",
    "\n",
    "</font>\n",
    "\n",
    "<img align=\"center\" src=\"https://github.com/psatencio/intro_keras/blob/master/figures/backprop.png?raw=true\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Utility functions\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_data(data_type, noise=0.2):\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    if data_type == 'moons':\n",
    "        X, Y = datasets.make_moons(200, noise=noise)\n",
    "    elif data_type == 'circles':\n",
    "        X, Y = sklearn.datasets.make_circles(200, noise=noise)\n",
    "    elif data_type == 'blobs':\n",
    "        X, Y = sklearn.datasets.make_blobs(centers=2, cluster_std=noise)\n",
    "    return X, Y\n",
    "\n",
    "def predict(W,b,X):\n",
    "    z = linear_activation(W,b,X)\n",
    "    A = sigmoid(z)\n",
    "    return np.round(A)\n",
    "    #return A\n",
    "\n",
    "def visualize_lr(W, b, X, y):\n",
    "    \n",
    "    X = X.T\n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    # Predict the function value for the whole gid\n",
    "    Z = predict(W,b,np.c_[xx.ravel(), yy.ravel()].T)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    color = ['blue' if y == 1 else 'red' for y in np.squeeze(Y)]\n",
    "    \n",
    "    return (X, color, xx, yy, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = generate_data('blobs', 1.5)\n",
    "Y = Y.reshape(1,len(Y))\n",
    "\n",
    "color = ['blue' if y == 1 else 'red' for y in np.squeeze(Y)] # una lista para darle color a las clases\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(X[:,0], X[:,1], color=color)\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.grid()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "X = X.T\n",
    "\n",
    "print(\"Size(X): \", X.shape)\n",
    "print(\"Size(Y): \", Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Finalmente, el descenso del gradiente en implementación vectorizada queda definido como:\n",
    "\n",
    "<br>\n",
    "\n",
    "<i>\n",
    "    \n",
    "__FOR__ $i=1 \\rightarrow$ epochs:\n",
    "\n",
    "    //forward propagation\n",
    "    Z = W.T*X+b\n",
    "    A = sig(Z)\n",
    "    \n",
    "    //cálculo de los gradientes de los parámetros\n",
    "    dz = A-Y\n",
    "    dW = (X*dz.T) / m\n",
    "    db = sum(dz) / m\n",
    "    \n",
    "    //cálculo del costo\n",
    "    J = cost(loss(Y,A))\n",
    "    \n",
    "    //actualizacion de parametros\n",
    "    W = W - learning_rate*dW\n",
    "    b = b - learning_rate*db\n",
    "__END FOR__\n",
    "    \n",
    "</i>\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "Implementemos el proceso completo con descenso del gradiente, utilizando los conceptos de vectorization y brocasting. Para ello, generemos primero un dataset sintético de clasificación binaria mediante las siguientes funciones utilitarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regresor logistico + Descenso del gradiente\n",
    "\n",
    "np.random.seed(2)\n",
    "\n",
    "#1.  Parametros del dataset\n",
    "m = len(X) #numero de ejemplos en el dataset.\n",
    "nx = X.shape[0] #dimensionalidad de la entrada o numero de features.\n",
    "\n",
    "#1. Parametros del regresor\n",
    "W = np.random.random([nx, 1]) # W tiene una tamanio de (nx, 1)\n",
    "b = np.random.random()\n",
    "\n",
    "print(\"W inicial: \", W)\n",
    "print(\"b inicial: \", b)\n",
    "\n",
    "#2 . Parametros del descenso del gradiente\n",
    "learning_rate = 0.005\n",
    "num_epochs = 100\n",
    "\n",
    "#3. Estructuras utilitarias\n",
    "history = [] #lista para almacenar el valor de cada \n",
    "\n",
    "for i in range(num_epochs): #1000 iteraciones del descenso del gradiente\n",
    "    '''\n",
    "    Activacion hacia adelante\n",
    "    '''\n",
    "    Z = linear_activation(W,b,X)\n",
    "    A = sigmoid(Z)\n",
    "    \n",
    "    '''\n",
    "    Activacion hacia atras - retropropagacion del error\n",
    "    '''\n",
    "    dz = None #A-Y\n",
    "    dW = None #dW = (X*dz.T) / m --- para el producto punto entre A y B se utiliza np.dot(A,B). A.T = A transpuesta.\n",
    "    db = None #sum(dz) / m --- sum == np.sum\n",
    "    \n",
    "    '''\n",
    "    Actualizacion de parametros mediante descenso del gradiente\n",
    "    '''\n",
    "    W -= None\n",
    "    b -= None\n",
    "    \n",
    "    l = logloss(Y,A)\n",
    "    J = cost(l)\n",
    "\n",
    "    history.append(J)\n",
    "\n",
    "print(\"W actualizado: \",W)\n",
    "print(\"b actualizado: \",b)\n",
    "print(\"costo total: \", J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(history)\n",
    "plt.xlabel(\"epoch / 10\")\n",
    "plt.ylabel(\"cost == total loss\")\n",
    "plt.grid()\n",
    "\n",
    "(X_pred, color, xx, yy, Z) = visualize_lr(W, b, X, Y)\n",
    "plt.subplot(122)\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.bone)\n",
    "plt.scatter(X_pred[:, 0], X_pred[:, 1], cmap=plt.cm.Spectral, c=color)\n",
    "plt.xlabel(\"feature 1\")\n",
    "plt.ylabel(\"feature 2\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "<font size=4>\n",
    "    \n",
    "__Analicemos__: \n",
    "\n",
    "1. Observe que el valor de la tasa de aprendizaje (learning_rate) es 0.005. Qué sucede con la convergencia del algoritmo si utilizamos un valor mayor, por ejemplo, $\\alpha = 0.05$ o un valor menor, por ejemplo, $\\alpha=1E-5$?\n",
    "\n",
    "\n",
    "2. En los scripts utilitarios modifique la función __predict__ para que no redondee la predicción. Que puede observar?\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img align=\"center\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/5/53/UNAL_Aplicación_Medell%C3%ADn.svg/1280px-UNAL_Aplicación_Medell%C3%ADn.svg.png\" width=\"300\"/></th>\n",
    "    <th><img align=\"center\" src=\"http://www.redttu.edu.co/es/wp-content/uploads/2016/01/itm.png\" width=\"300\"/> </th> \n",
    "    <th><img align=\"center\" src=\"https://www.cienciasdelaadministracion.uns.edu.ar/wp-content/themes/enlighten-pro/images/logo-uns-horizontal.png\" width=\"300\"/></th>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "#### Pedro Atencio Ortiz - 2019 (pedroatencio@itm.edu.co)\n",
    "\n",
    "# Módulo 2.1. Introducción a Keras\n",
    "\n",
    "En este notebook abordaremos los siguientes tópicos:\n",
    "\n",
    "1. Primera red en Keras \n",
    "2. Ensamble de la red: a) Construcción como lista b) Agregación de capas (model.add) b) \"Cableado\" manual.\n",
    "3. Compilación\n",
    "4. Preparacion del dataset.\n",
    "    1. Split.\n",
    "    2. K-Fold.\n",
    "5. Entrenamiento y validación.\n",
    "    1. model.evaluate()\n",
    "    2. evaluacion durante el entrenamiento.\n",
    "6. Ejemplo: MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones utilitarias\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def generate_data(data_type, noise=0.2, num_samples=200):\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    if data_type == 'moons':\n",
    "        X, Y = datasets.make_moons(num_samples, noise=noise)\n",
    "    elif data_type == 'circles':\n",
    "        X, Y = sklearn.datasets.make_circles(num_samples, noise=noise)\n",
    "    elif data_type == 'blobs':\n",
    "        X, Y = sklearn.datasets.make_blobs(centers=2, cluster_std=noise)\n",
    "    return X, Y\n",
    "\n",
    "def visualize_model(model, X, Y, output='truncate'):\n",
    "    XT = np.copy(X)\n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = XT[:, 0].min() - .5, XT[:, 0].max() + .5\n",
    "    y_min, y_max = XT[:, 1].min() - .5, XT[:, 1].max() + .5\n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    # Predict the function value for the whole gid\n",
    "    if(output=='truncate'):\n",
    "        Z = np.round(model.predict(np.c_[xx.ravel(), yy.ravel()]))\n",
    "    elif(output=='same'):\n",
    "        Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    else:\n",
    "        print(\"output param must be either truncate or same\")\n",
    "        return False\n",
    "\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot the contour and training examples\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.bone)\n",
    "\n",
    "    color = ['blue' if y == 1 else 'red' for y in np.squeeze(Y)]\n",
    "    plt.scatter(X[:,0], X[:,1], color=color)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 1. Primera red en Keras\n",
    "\n",
    "Para este ejercicio, utilizaremos Keras para construir la misma red neuronal del módulo anterior e igualmente entrenaremos la misma mediante los optimizadores de la librería.\n",
    "\n",
    "<img align=\"center\" src=\"https://github.com/psatencio/intro_keras/blob/master/figures/layered_net.png?raw=true\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "Y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "nx = X.shape[0]\n",
    "m = len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En keras, la capa de tipo __Dense__ equivale a una capa totalmente conectada (fully connected), es decir, una capa que recibe como entrada todas las salidas de la capa anterior, y entrega todas sus salidas a la siguiente capa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.backend.clear_session() #borra el grafo de la sesion. Util cuando creamos muchos modelos en una sesion.\n",
    "\n",
    "# Red neuronal\n",
    "model = Sequential()\n",
    "model.add(Dense(units=3, input_dim=2, activation='sigmoid', use_bias=True)) #capa 1. La dimensionalidad de la entrada solo se define para la primera capa\n",
    "model.add(Dense(units=1, activation='sigmoid', use_bias=True)) #capa 2\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy', # funcion loss\n",
    "              metrics=['accuracy']) # metricas complementarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X,Y, epochs=3000, verbose=0)\n",
    "\n",
    "print(\"Error (Loss) final: %.4E\"%(np.array(history.history['loss'][-1:]))) #Error final de la lista de errores\n",
    "print(\"Precision (Accuracy) final: %.2f\"%(np.array(history.history['acc'][-1:])))\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['acc'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_model(model, X, Y, output='truncate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Trabajemos\n",
    "\n",
    "<br>\n",
    "\n",
    "<font size=4>\n",
    "\n",
    "1. Utilicemos lo implementado al momento para clasificar los problemas __'moons'__ y __'circles'__ (ver figura siguiente), y probar:\n",
    "\n",
    "<br>\n",
    "\n",
    "<ul>\n",
    "\n",
    "<li>Pruebe agregando más neuronas en la capa 1.</li>\n",
    "<li>Pruebe agregando más capas a la red.</li>\n",
    "<li>Pruebe distintas configuraciones $\\alpha$ (learning rate).</li>\n",
    "<li>Pruebe distintos valores para el número de épocas.</li>\n",
    "</ul>\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = generate_data('moons', 0.1)\n",
    "\n",
    "color = ['blue' if y == 1 else 'red' for y in np.squeeze(Y)] # una lista para darle color a las clases\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(X[:,0], X[:,1], color=color)\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 2. Ensamble de la red\n",
    "\n",
    "Dependiendo del nivel de especificidad deseado para la arquitectura de la red neuronal, podemos utilizar diferentes formas de construirla (ver figura).\n",
    "\n",
    "<img align=\"center\" src=\"https://github.com/psatencio/intro_keras/blob/master/figures/build_strategies.png?raw=true\" width=\"500\"/>\n",
    "\n",
    "En la figura anterior, podemos observar tres formas de construir una misma red neuronal:\n",
    "\n",
    "1. Ingresando las capas como una lista (A).\n",
    "2. Agregando las capas mediante el módulo add() (B).\n",
    "3. Configurando el gráfo de cómputo (C).\n",
    "\n",
    "A continuación, implementemos cada caso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Como lista\n",
    "modelA = Sequential([Dense(units=2, input_dim=2, activation='sigmoid'),\n",
    "                   Dense(units=1, activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizando el modulo add (agregar)\n",
    "modelB = Sequential()\n",
    "modelB.add(Dense(units=2, input_dim=2, activation='sigmoid'))\n",
    "modelB.add(Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Cableando\" manualmente las capas\n",
    "\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "x = Input(shape=(2,))\n",
    "a1 = Dense(units=2, activation='sigmoid')(x)\n",
    "a2 = Dense(units=1, activation='sigmoid')(a1)\n",
    "\n",
    "modelC = Model(inputs=x, outputs=a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelC.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy', # funcion loss\n",
    "              metrics=['accuracy']) # metricas complementarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = modelC.fit(X,Y, epochs=200, verbose=0)\n",
    "\n",
    "print(\"Error (Loss) final: %.4E\"%(np.array(history.history['loss'][-1:]))) #Error final de la lista de errores\n",
    "print(\"Precision (Accuracy) final: %.2f\"%(np.array(history.history['acc'][-1:])))\n",
    "\n",
    "#plt.plot(history.history['loss'])\n",
    "#plt.plot(history.history['acc'])\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_model(modelC, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Discutamos\n",
    "\n",
    "<br>\n",
    "\n",
    "<font size=4>\n",
    "\n",
    "- En qué situaciones conviene una aproximación u otra?\n",
    "- Agreguemos una nueva capa al modelo de ejemplo e implementemos el mismo en cada una de las formas (A,B y C).\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 3. Compilación\n",
    "\n",
    "La compilación permite definir los elementos del entrenamiento del modelo, particularmente, permite configurar:\n",
    "\n",
    "- <a href=\"https://keras.io/optimizers/\">El optimizador</a>.\n",
    "- <a href=\"https://keras.io/losses/\">La función de error</a>.\n",
    "- <a href=\"https://keras.io/metrics/\">Las métricas de evaluación</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset\n",
    "\n",
    "X, Y = generate_data('circles', 0.1)\n",
    "\n",
    "color = ['blue' if y == 1 else 'red' for y in np.squeeze(Y)] # una lista para darle color a las clases\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(X[:,0], X[:,1], color=color)\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session() #borra el grafo de la sesion. Util cuando creamos muchos modelos en una sesion.\n",
    "\n",
    "#Construccion de la red\n",
    "model = Sequential([Dense(units=5, input_shape=(2, ) , activation='sigmoid'),\n",
    "                   Dense(units=1, activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Podemos especificar con alto nivel de detalle cada elemento de la compilacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizador = tf.keras.optimizers.RMSprop(lr=0.1) #objeto optimizador de tipo RMSprop\n",
    "error = tf.keras.losses.binary_crossentropy\n",
    "area_roc = tf.keras.metrics.AUC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos crear una métrica propia. Para ello debemos crear una función con los parámetros de entrada $(y\\_true, y\\_pred)$, donde $y\\_true$ es un tensor de las categorías reales de $X$ y $y\\_pred$, un tensor con las predicciones realizadas por el modelo.\n",
    "\n",
    "Por otra parte, las operaciones al interior de la función deben estar construidas utilizando las funciones del backend de Keras __tensorflow.keras.backend__ o de tensorflow.\n",
    "\n",
    "__Nota__: En caso de ser necesario crear una métrica que no dependa exclusivamente de $(y\\_true, y\\_pred)$, es necesario utilizar __clausura__ de funciones (<a href=\"https://es.wikipedia.org/wiki/Clausura_(informática)\">function closure</a>). Para ahondar en el tema, revisar el siguiente <a href=\"https://towardsdatascience.com/advanced-keras-constructing-complex-custom-losses-and-metrics-c07ca130a618\">enlace</a>.\n",
    "\n",
    "Supongamos que queremos mostrar la precisión entre 0 y 100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def metrica_propia(y_true, y_pred):\n",
    "    '''\n",
    "    Metrica de prueba. Devuelve el accuracy en porcentaje.\n",
    "    '''\n",
    "    y = K.round(y_true)\n",
    "    a = K.round(y_pred)\n",
    "    \n",
    "    return (K.constant(1) - K.mean(K.abs(y-a))) * K.constant(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, utilizamos la función __compile__ predefinida en los modelos de Keras y enviamos los siguientes argumentos a los parámetros en esta forma:\n",
    "\n",
    "- __optimizer__ : Optimizador (instancia) o string predefinido. <a href=\"https://keras.io/optimizers/\">tensorflow.keras.optimizers</a>.\n",
    "- __loss__: Función de error (función) o string predefinido. <a href=\"\">tensorflow.keras.losses</a>\n",
    "- __metrics__: <font color=\"red\">Lista</font> de funciones de métricas, o de strings de métricas predefinidas. <a href=\"https://keras.io/metrics/\">tensorflow.keras.metrics</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizador, loss=error, metrics=['acc', area_roc, metrica_propia])\n",
    "\n",
    "history = model.fit(X,Y, epochs=200, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['auc'])\n",
    "\n",
    "plt.legend(['loss', 'Accuracy','Area ROC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Error (Loss) final: %.4E\"%(np.array(history.history['loss'][-1:]))) #Error final de la lista de errores\n",
    "\n",
    "print(\"Precision (Accuracy) final: %.2f\"%(np.array(history.history['acc'][-1:])))\n",
    "print(\"Area debajo de la curva ROC final: %.2f\"%(np.array(history.history['auc'][-1:])))\n",
    "print(\"Precision entre 0-100 final: %.2f\"%(np.array(history.history['metrica_propia'][-1:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_model(model, X, Y, output='same')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 4. Preparación del dataset\n",
    "\n",
    "En cualquier aproximacion de aprendizaje de maquina es necesario validar el modelo utilizando alguna estrategia de separacion de datos, por ejemplo, split o k-fold (ver figura).\n",
    "\n",
    "<img align=\"center\" src=\"https://github.com/psatencio/intro_keras/blob/master/figures/train_test.png?raw=true\" width=\"500\"/>\n",
    "\n",
    "En esta seccion trabajaremos sobre estas aproximaciones utilizando arreglos de Numpy para manipular los datasets.\n",
    "\n",
    "__Nota:__ Al trabajar con datasets con grandes cantidades de ejemplos, resulta inviable en muchos casos utilizar arreglos de Numpy que requieren cargarse en memoria. Para ello es posible utilizar <a href=\"https://towardsdatascience.com/keras-data-generators-and-how-to-use-them-b69129ed779c\">generadores</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset split\n",
    "\n",
    "Separemos nuestro dataset en dos conjuntos, uno para entrenar y otro para evaluar, 70% / 30% por ejemplo. Para ello debemos primero asegurar que las clases estén uniformemente distribuida, por ejemplo, permutando los indices del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "\n",
    "X, Y = generate_data('circles', 0.1, num_samples=300)\n",
    "\n",
    "m = len(X)\n",
    "\n",
    "print(\"El dataset tiene %i ejemplos.\"%(m))\n",
    "\n",
    "indices = np.arange(m) #creamos los indices ordenados del dataset\n",
    "print(\"Indices: \",indices,\"\\n\")\n",
    "\n",
    "indices_permutados = np.random.permutation(indices)\n",
    "print(\"Indices permutados: \",indices_permutados,\"\\n\")\n",
    "\n",
    "#np.random.shuffle(indices)\n",
    "#print(\"Indices permutados: \",indices,\"\\n\")\n",
    "\n",
    "train_fraction = 0.8\n",
    "train_index = indices_permutados[:int(m*train_fraction)]\n",
    "test_index = indices_permutados[int(m*train_fraction):]\n",
    "\n",
    "#print(\"Indices de entrenamiento: \", train_indices, \"\\n\")\n",
    "#print(\"Indices de prueba: \", test_indices, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train,Y_train) = (X[train_index], Y[train_index])\n",
    "(X_test,Y_test) = (X[test_index], Y[test_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold\n",
    "\n",
    "Utilicemos la función k-fold de sklearn para ello. Esta función nos entrega los indices en cada iteración de $k$, tanto para el entrenamiento como para la prueba. El desempeño final de nuestro modelo será el promedio de los errores de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = generate_data('circles', 0.1, num_samples=200)\n",
    "\n",
    "kf = KFold(n_splits=k, shuffle=True)\n",
    "kf.get_n_splits(X)\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    print(\"Iteracion: %i \\n\"%(i))\n",
    "    print(\"Train set: \",train_index, \"\\n\")\n",
    "    print(\"Test set: \",test_index, \"\\n\")\n",
    "    \n",
    "    # En este punto se entrena y evalua el modelo k veces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 5. Entrenamiento y validación\n",
    "\n",
    "Para ello podemos probar / validar el modelo posterior al entrenamiento o durante el entrenamiento. En el primer caso utilizaremos __model.predict__ y en el segundo caso utilizaremos los parametros de validación dentro de la función __model.fit__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación posterior al entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creemos y compilemos nuestra red neuronal\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = Sequential([Dense(units=10, input_dim=2, activation='tanh', use_bias=True),\n",
    "                   Dense(units=1, activation='sigmoid', use_bias=True)])\n",
    "\n",
    "optimizador = RMSprop(lr=0.01)\n",
    "model.compile(optimizer=optimizador, loss='binary_crossentropy', metrics=['acc', 'AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrenemos el modelo\n",
    "history = model.fit(x=X_train, y=Y_train, epochs=1000, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.xlabel(\"epoca\")\n",
    "plt.ylabel(\"magnitud\")\n",
    "plt.legend([\"loss\"])\n",
    "plt.show()\n",
    "\n",
    "visualize_model(model, X_train, Y_train, output='truncate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluemos el modelo\n",
    "\n",
    "resultado = model.evaluate(x=X_test, y=Y_test)\n",
    "print(\"Loss: %.4f and metrics: \"%(resultado[0]), resultado[1:])\n",
    "\n",
    "visualize_model(model, X_test, Y_test, output='truncate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación durante el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creemos y compilemos nuestra red neuronal\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = Sequential([Dense(units=5, input_dim=2, use_bias=True), Activation(activation='elu'), \n",
    "                   Dense(units=1, activation='sigmoid', use_bias=True)])\n",
    "\n",
    "optimizador = RMSprop(lr=0.001)\n",
    "model.compile(optimizer=optimizador, loss='binary_crossentropy', metrics=['acc', 'AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Incluimos los datos de prueba en el parametro validation_data\n",
    "\n",
    "history = model.fit(x=X_train, y=Y_train, validation_data=[X_test, Y_test] , epochs=1000, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.xlabel(\"epoca\")\n",
    "plt.ylabel(\"magnitud\")\n",
    "plt.legend([\"loss\", \"val_loss\"])\n",
    "plt.show()\n",
    "\n",
    "visualize_model(model, X_train, Y_train, output='truncate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 6. Ejemplo MNIST\n",
    "\n",
    "Utilicemos lo anteriormente tratado para implementar un modelo para el problema de clasificación de digitos del dataset MNIST. Para ello:\n",
    "\n",
    "1. Analice el dataset.\n",
    "2. Construya un dataset donde cada sample sea un arreglo de 28x28.\n",
    "3. Construya un modelo de clasificación.\n",
    "4. Entrene y valide.\n",
    "5. Ajuste los parámetros (número de neuronas, épocas, capas).\n",
    "6. Mida el desempeño del modelo utilizando k-fold.\n",
    "7. Muestre ejemplos de clasificación correcta e incorrecta utilizando __model.predict()__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "print(x_train.shape)\n",
    "plt.imshow(x_train[0,:], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

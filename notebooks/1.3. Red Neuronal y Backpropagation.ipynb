{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img align=\"center\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/5/53/UNAL_Aplicación_Medell%C3%ADn.svg/1280px-UNAL_Aplicación_Medell%C3%ADn.svg.png\" width=\"300\"/></th>\n",
    "    <th><img align=\"center\" src=\"http://www.redttu.edu.co/es/wp-content/uploads/2016/01/itm.png\" width=\"300\"/> </th> \n",
    "    <th><img align=\"center\" src=\"https://www.cienciasdelaadministracion.uns.edu.ar/wp-content/themes/enlighten-pro/images/logo-uns-horizontal.png\" width=\"300\"/></th>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "#### Pedro Atencio Ortiz - 2019\n",
    "\n",
    "# 3. Red Neuronal y Backpropagation (descenso del gradiente generalizado)\n",
    "\n",
    "En este notebook abordaremos los siguientes tópicos:\n",
    "\n",
    "- Notación.\n",
    "- Forward propagation.\n",
    "- Backpropagation.\n",
    "- Errores y funciones de activación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones utilitarias\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_data(data_type, noise=0.2):\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    if data_type == 'moons':\n",
    "        X, Y = datasets.make_moons(200, noise=noise)\n",
    "    elif data_type == 'circles':\n",
    "        X, Y = sklearn.datasets.make_circles(200, noise=noise)\n",
    "    elif data_type == 'blobs':\n",
    "        X, Y = sklearn.datasets.make_blobs(centers=2, cluster_std=noise)\n",
    "    return X, Y\n",
    "\n",
    "def visualize_lr(W1, b1, W2, b2, W3, b3, X, Y):\n",
    "    X = X.T\n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    # Predict the function value for the whole gid\n",
    "    Z = predict_multilayer(W1,b1,W2,b2,W3,b3,np.c_[xx.ravel(), yy.ravel()].T)\n",
    "\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot the contour and training examples\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.bone)\n",
    "    \n",
    "    color = ['blue' if y == 1 else 'red' for y in np.squeeze(Y)]\n",
    "    plt.scatter(X[:,0], X[:,1], color=color)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 3.1. Notación\n",
    "\n",
    "Con el objetivo de poder trabajar un mecanismo general de entrenamiento de una secuencia de regresores, llamados de ahora en adelante red neuronal, necesitamos primero definir una notación genérica para el dataset, parámetros de la red neuronal y resultados de esta. Analicemos la siguiente figura:\n",
    "\n",
    "<img align=\"center\" src=\"https://github.com/psatencio/intro_keras/blob/master/figures/layered_regresor_general.png?raw=true\" width=\"500\"/>\n",
    "\n",
    "Nótese que tanto el conjunto de datos de entrada $X$ como los pesos $W$ y demás elementos pueden definirse como matrices y arreglos, que al ser operados mediante vectorization / broadcasting, nos permiten simplificar el proceso de construcción y cálculo.\n",
    "\n",
    "A partir de lo anterior, definamos los siguientes elementos:\n",
    "\n",
    "- Sea $[X,Y]$ un dataset supervisado que contiene $m$ ejemplos, cada uno de dimensión $n_x$, entonces $X_{(n_x, m)}$, $Y_{(1, m)}$.\n",
    "\n",
    "<img align=\"center\" src=\"https://github.com/psatencio/intro_keras/blob/master/figures/dataset.png?raw=true\" width=\"500\"/>\n",
    "\n",
    "- Sea $l$ el número de capas de la red y $n^{[i]}$ el numero de neuronas de la capa $i$.\n",
    "- Sea $W^{[i]}$ la matriz de pesos de la capa $i$, entonces $W^{[i]}_{(n^{[i]},n^{[i-1]})}$. Nótese que esto es equivalente a definir $W$ de forma traspuesta.\n",
    "- Sea $b^{[i]}$ el arreglo de bias de la capa $i$, entonces $b^{[i]}_{(n^{[i]}, 1)}$.\n",
    "- Sea $Z^{[i]}$ la activacion lineal o entrada de la capa $i$, entonces $Z^{[i]}_{(n^{[i]}, m)}$\n",
    "- Sea $A^{[i]}$ la salida de la capa $i$, entonces $A^{[i]}_{(n^{[i]}, m)}$. Nótese que $A^{[0]} = X$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 3.2. Forward Propagation\n",
    "\n",
    "Este es el proceso de computar la salida de la red a partir de la entrada, en otras palabras, calcular las predicciones para un conjunto de entrada.\n",
    "\n",
    "Este proceso se utiliza principalmente en dos momentos:\n",
    "\n",
    "1. En el entrenamiento de la red.\n",
    "2. Una vez entrenada, para lanzar predicciones.\n",
    "\n",
    "Se define de forma general mediante las siguientes instrucciones:\n",
    "\n",
    "<br>\n",
    "\n",
    "<font size=3>\n",
    "$\n",
    "\\\\\n",
    "para(i:1 \\rightarrow l)\\{\n",
    "\\\\\n",
    "\\hspace{10mm} Z^{[i]} = W^{[i]}.A^{[i-1]} + b^{[i]}\n",
    "\\\\\n",
    "\\hspace{10mm} A^{[i]} = f(A^{[i]})\n",
    "\\\\\n",
    "\\}\n",
    "$\n",
    "</font>\n",
    "\n",
    "Implementemos lo anterior para el caso del regresor en cadena que utilizamos para el XOR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "Y = np.array([[0, 1, 1, 0]])\n",
    "\n",
    "color= ['blue' if y == 1 else 'red' for y in np.squeeze(Y)]\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(X[:,0], X[:,1], color=color)\n",
    "plt.grid()\n",
    "plt.xlabel(r'$p$', fontsize=12)\n",
    "plt.ylabel(r'$q$', fontsize=12)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "X = X.T #Transponemos X para que quede de dimension (nx, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definamos los W en cada caso\n",
    "np.random.seed(2)\n",
    "W1 = np.random.random([2,2])\n",
    "b1 = np.zeros([2,1])\n",
    "W2 = np.random.random([1,2])\n",
    "b2 = np.zeros([1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    A = 1. / (1. + np.exp(-Z))  \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salida de la red para 4 datos:  [[0.59275141 0.60191942 0.61413859 0.6220829 ]]\n"
     ]
    }
   ],
   "source": [
    "#Feed-Forward: implementamos el ciclo explicitamente\n",
    "Z1 = np.dot(W1,X) + b1\n",
    "A1 = sigmoid(Z1)\n",
    "Z2 = np.dot(W2, A1) + b2\n",
    "A2 = sigmoid(Z2)\n",
    "\n",
    "print(\"Salida de la red para 4 datos: \", A2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salida de la red para 4 datos:  [[0.59275141 0.60191942 0.61413859 0.6220829 ]]\n"
     ]
    }
   ],
   "source": [
    "#Este proceso se puede automatizar utilizando alguna estructura de datos, por ejemplo, diccionarios\n",
    "\n",
    "layers = {\"layer1\":{\"W\":W1, \"b\":b1}, \"layer2\":{\"W\":W2, \"b\":b2}} #parametros de la red neuronal\n",
    "pred = {\"layer0\":{\"A\":X}} #diccionario para guardar los Z y A en cada iteracion del feed-forward\n",
    "\n",
    "for i in range(1, len(layers)+1):\n",
    "    layer = layers[\"layer\"+str(i)]\n",
    "    pred[\"layer\"+str(i)] = {}\n",
    "    \n",
    "    #Recuperamos datos de la iteracion anterior\n",
    "    W = layer[\"W\"]\n",
    "    b = layer[\"b\"]\n",
    "    A_prev = pred[\"layer\"+str(i-1)][\"A\"]\n",
    "    \n",
    "    #Feed-forward\n",
    "    Z = np.dot(W, A_prev) + b\n",
    "    A = sigmoid(Z)\n",
    "    \n",
    "    #Salvamos los datos de esta iteracion\n",
    "    pred[\"layer\"+str(i)][\"Z\"] = Z\n",
    "    pred[\"layer\"+str(i)][\"A\"] = A\n",
    "\n",
    "print(\"Salida de la red para 4 datos: \", pred[\"layer2\"][\"A\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 3.3. Backpropagation + update\n",
    "\n",
    "Backpropagation o retro-propagación del error consiste en aplicar el descenso del gradiente sobre una red neuronal con una __arquitectura arbitraria__, utilizando para ello la regla de la cadena sobre el gráfo de cómputo de la red para obtener las derivadas parciales de cada parámetro a partir del error en la salida.\n",
    "\n",
    "<img align=\"center\" src=\"https://github.com/psatencio/intro_keras/blob/master/figures/backprop_general.png?raw=true\" width=\"500\"/>\n",
    "\n",
    "Se define de forma general mediante las siguientes instrucciones:\n",
    "\n",
    "<br>\n",
    "\n",
    "<font size=3>\n",
    "$\n",
    "\\\\\n",
    "para(i:l \\rightarrow 1)\\{\n",
    "\\\\\n",
    "\\hspace{10mm} dZ^{[i]} = \\left\\{\\begin{matrix} si(i==l) & A^{[i]} - Y \\\\ sino & W^{[i+1]T}.dZ^{[i+1]} \\end{matrix}\\right.\n",
    "\\\\\n",
    "\\hspace{10mm} dW^{[i]} = \\frac{(dZ^{[i]}.A^{[i-1]T})}{m}\n",
    "\\\\\n",
    "\\hspace{10mm} db^{[i]} = \\frac{\\sum{(dZ^{[i]})}}{m}\n",
    "\\\\\n",
    "\\}\n",
    "$\n",
    "\n",
    "$\n",
    "\\\\\n",
    "para(i:1 \\rightarrow l)\\{\n",
    "\\\\\n",
    "\\hspace{10mm} W^{[i]} = W^{[i]} - \\alpha dW^{[i]}\n",
    "\\\\\n",
    "\\hspace{10mm} b^{[i]} = b^{[i]} - \\alpha db^{[i]}\n",
    "\\\\\n",
    "\\}\n",
    "$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = generate_data('moons', 0.1)\n",
    "Y = Y.reshape(1,len(Y))\n",
    "\n",
    "color = ['blue' if y == 1 else 'red' for y in np.squeeze(Y)] # una lista para darle color a las clases\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(X[:,0], X[:,1], color=color)\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Revisemos el operador y trabajemos sobre sus tablas de verdad:\n",
    "\n",
    "<br>\n",
    "\n",
    "<center><font size=5>$p \\oplus q = (p \\wedge \\neg q) \\vee (\\neg p \\wedge q) $</font></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "Y1 = np.array([[0, 1, 1, 1]])\n",
    "\n",
    "X2 = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "Y2 = np.array([[1, 1, 1, 0]])\n",
    "\n",
    "X3 = np.array([[0,1],[1,1],[1,1],[1,0]])\n",
    "Y3 = np.array([[0, 1, 1, 0]])\n",
    "\n",
    "color1 = ['blue' if y == 1 else 'red' for y in np.squeeze(Y1)]\n",
    "color2 = ['blue' if y == 1 else 'red' for y in np.squeeze(Y2)]\n",
    "color3 = ['blue' if y == 1 else 'red' for y in np.squeeze(Y3)]\n",
    "\n",
    "plt.figure(figsize=(15,4))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.title(r'L = $(p \\wedge \\neg q)$')\n",
    "plt.scatter(X1[:,0], X1[:,1], color=color1)\n",
    "plt.grid()\n",
    "plt.xlabel(r'$p$', fontsize=12)\n",
    "plt.ylabel(r'$q$', fontsize=12)\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.title(r'$R = (\\neg p \\wedge q)$')\n",
    "plt.scatter(X2[:,0], X2[:,1], color=color2)\n",
    "plt.grid()\n",
    "plt.xlabel(r'$p$', fontsize=12)\n",
    "plt.ylabel(r'$q$', fontsize=12)\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.title(r'$L \\vee R$')\n",
    "plt.scatter(X3[:,0], X3[:,1], color=color3)\n",
    "plt.grid()\n",
    "plt.xlabel(r'$L$', fontsize=12)\n",
    "plt.ylabel(r'$R$', fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "El análisis anterior permite suponer que es posible entonces utilizar tres regresores logísticos para aprender cada una de las tres operaciones anteriores y concatenarlos de tal forma que permitan predecir el operador XOR a partir de las entradas.\n",
    "\n",
    "Para ello, entrenaremos cada regresor por separado utilizando las tablas de verdad construidas en cada caso y finalmente los concatenaremos en una sola estructura como se puede observar en la siguiente figura:\n",
    "\n",
    "<img align=\"center\" src=\"https://github.com/psatencio/intro_keras/blob/master/figures/layered_regresor.png?raw=true\" width=\"500\"/>\n",
    "\n",
    "<br>\n",
    "\n",
    "Analicemos el siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utilicemos las mismas operaciones vectorizadas para los regresores logisticos.\n",
    "\n",
    "def linear_activation(W, b, X):\n",
    "    z = np.dot(W.T,X) + b\n",
    "    \n",
    "    return z\n",
    "\n",
    "def sigmoid(z):\n",
    "    a = 1. / (1. + np.exp(-z)) \n",
    "    \n",
    "    return a \n",
    "\n",
    "def logloss(y, a):\n",
    "    return -(y * np.log(a) + (1-y) * np.log(1-a))\n",
    "\n",
    "def cost(L):\n",
    "    return np.mean(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets, 1 por regresor\n",
    "\n",
    "X1 = np.array([[0,0],[0,1],[1,0],[1,1]]).T\n",
    "Y1 = np.array([[0, 1, 1, 1]])\n",
    "\n",
    "X2 = np.array([[0,0],[0,1],[1,0],[1,1]]).T\n",
    "Y2 = np.array([[1, 1, 1, 0]])\n",
    "\n",
    "X3 = np.array([[0,1],[1,1],[1,1],[1,0]]).T\n",
    "Y3 = np.array([[0, 1, 1, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametros de los regresores 1, 2 y 3\n",
    "np.random.seed(2)\n",
    "\n",
    "m = len(X1)\n",
    "nx = X1.shape[0]\n",
    "\n",
    "W1 = np.random.random([nx, 1]) \n",
    "b1 = np.random.random()\n",
    "\n",
    "W2 = np.random.random([nx, 1]) \n",
    "b2 = np.random.random()\n",
    "\n",
    "W3 = np.random.random([nx, 1])\n",
    "b3 = np.random.random()\n",
    "\n",
    "# Parametros del descenso del gradiente\n",
    "num_epochs = 1000\n",
    "learning_rate = 0.1\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    # Computacion de las activaciones de cada regresor\n",
    "    Z1 = linear_activation(W1,b1,X1)\n",
    "    A1 = sigmoid(Z1)\n",
    "    \n",
    "    Z2 = linear_activation(W2,b2,X2)\n",
    "    A2 = sigmoid(Z2)\n",
    "    \n",
    "    Z3 = linear_activation(W3,b3,X3)\n",
    "    A3 = sigmoid(Z3)\n",
    "    \n",
    "    # Calculo de gradientes\n",
    "    dZ1 = A1 - Y1\n",
    "    dW1 = np.dot(X1, dZ1.T) / m\n",
    "    db1 = np.sum(dZ1) / m\n",
    "    \n",
    "    dZ2 = A2 - Y2\n",
    "    dW2 = np.dot(X2, dZ2.T) / m\n",
    "    db2 = np.sum(dZ2) / m\n",
    "    \n",
    "    dZ3 = A3 - Y3\n",
    "    dW3 = np.dot(X3, dZ3.T) / m\n",
    "    db3 = np.sum(dZ3) / m\n",
    "    \n",
    "    # Actualizacion de parametros\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    W3 -= learning_rate * dW3\n",
    "    b3 -= learning_rate * db3\n",
    "    \n",
    "    # Estimacion del costo\n",
    "    J1 = cost(logloss(Y1,A1))\n",
    "    J2 = cost(logloss(Y2,A2))\n",
    "    J3 = cost(logloss(Y3,A3))\n",
    "\n",
    "print(\"W1 actualizado: \",W1, \"b1 actualizado: \", b1, \" Costo final: \", J1)\n",
    "print(\"W2 actualizado: \",W2, \"b2 actualizado: \", b2, \" Costo final: \", J2)\n",
    "print(\"W3 actualizado: \",W3, \"b3 actualizado: \", b3, \" Costo final: \", J3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Si bien ahora tenemos nuestros regresor entrenados de manera independiente, debemos conectarlos para poder predecir el valor de salida desde la entrada a los regresores 1 y 2, posteriormente concatenar ambas salidas como entradas para el regresor 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_multilayer(W1,b1,W2,b2,W3,b3,X):\n",
    "    Z1 = linear_activation(W1,b1,X)\n",
    "    A1 = sigmoid(Z1)\n",
    "    \n",
    "    Z2 = linear_activation(W2,b2,X)\n",
    "    A2 = sigmoid(Z2)\n",
    "    \n",
    "    X3 = np.concatenate((A1,A2), axis=0) #En este punto concatenamos A1 y A2 como entradas para el regresor 3\n",
    "    Z3 = linear_activation(W3, b3, X3)\n",
    "    A3 = sigmoid(Z3)\n",
    "    \n",
    "    return A3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_lr(W1, b1, W2, b2, W3, b3, X, Y) #Grafiquemos el mapa de separacion de nuestros regresores en cadena."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "<font size=4>\n",
    "    \n",
    "__Analicemos__: \n",
    "\n",
    "Si bien este ejercicio muestra que un conjunto de regresores en cadena puede aproximar problemas no-lineales, finalmente, esta aproximación de entrenamiento es inviable en cualquier caso. En qué radica dicha inviabilidad?\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

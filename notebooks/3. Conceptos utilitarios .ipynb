{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img align=\"center\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/5/53/UNAL_Aplicación_Medell%C3%ADn.svg/1280px-UNAL_Aplicación_Medell%C3%ADn.svg.png\" width=\"300\"/></th>\n",
    "    <th><img align=\"center\" src=\"http://www.redttu.edu.co/es/wp-content/uploads/2016/01/itm.png\" width=\"300\"/> </th> \n",
    "    <th><img align=\"center\" src=\"https://www.cienciasdelaadministracion.uns.edu.ar/wp-content/themes/enlighten-pro/images/logo-uns-horizontal.png\" width=\"300\"/></th>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "#### Pedro Atencio Ortiz - 2019 (pedroatencio@itm.edu.co)\n",
    "\n",
    "# Módulo 3. Conceptos utilitarios\n",
    "\n",
    "En este notebook abordaremos los siguientes tópicos:\n",
    "\n",
    "1. Callbacks: Tomar decisiones durante el proceso de entrenamiento. \n",
    "2. Lamba layers: Construir nuestras propias capas de red neuronal.\n",
    "3. Estimación del $learning\\_rate$. \n",
    "4. Grid search: Encontrar los mejores parámetros de la red.\n",
    "5. Custom losses: Construir nuestras propias funciones de error.\n",
    "6. Custom Activations: Construir nuestras propias funciones de activación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones utilitarias\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def generate_data(data_type, noise=0.2, num_samples=200):\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    if data_type == 'moons':\n",
    "        X, Y = datasets.make_moons(num_samples, noise=noise)\n",
    "    elif data_type == 'circles':\n",
    "        X, Y = sklearn.datasets.make_circles(num_samples, noise=noise)\n",
    "    elif data_type == 'blobs':\n",
    "        X, Y = sklearn.datasets.make_blobs(centers=2, cluster_std=noise)\n",
    "    return X, Y\n",
    "\n",
    "def visualize_model(model, X, Y, output='truncate', save=False, save_path=None):\n",
    "    XT = np.copy(X)\n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = XT[:, 0].min() - .5, XT[:, 0].max() + .5\n",
    "    y_min, y_max = XT[:, 1].min() - .5, XT[:, 1].max() + .5\n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    # Predict the function value for the whole gid\n",
    "    if(output=='truncate'):\n",
    "        Z = np.round(model.predict(np.c_[xx.ravel(), yy.ravel()]))\n",
    "    elif(output=='same'):\n",
    "        Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    else:\n",
    "        print(\"output param must be either truncate or same\")\n",
    "        return False\n",
    "\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot the contour and training examples\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.bone)\n",
    "\n",
    "    color = ['blue' if y == 1 else 'red' for y in np.squeeze(Y)]\n",
    "    plt.scatter(X[:,0], X[:,1], color=color)\n",
    "\n",
    "    if(save):\n",
    "        plt.savefig(save_path)\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 1. Callbacks\n",
    "\n",
    "Los callbacks son funciones que nos permiten realizar tareas personalizadas durante el entrenamiento de la red neuronal. <a href=\"https://keras.io/callbacks/\">Keras </a> tiene un conjunto de callbacks predefinidos, y permite utilizar la clase abstracta __Callback__ para construidas callbacks personalizadas.\n",
    "\n",
    "Entre los callbacks personalizados más utilizados se encuentran:\n",
    "\n",
    "- EarlyStopping\n",
    "- LearningRateScheduler\n",
    "- TerminateOnNaN\n",
    "\n",
    "Probemos algunos de estos callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input, LeakyReLU, Activation\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = generate_data('moons', 0.3)\n",
    "\n",
    "m = len(X)\n",
    "\n",
    "indices = np.arange(m) #creamos los indices ordenados del dataset\n",
    "indices_permutados = np.random.permutation(indices)\n",
    "\n",
    "train_fraction = 0.8\n",
    "train_index = indices_permutados[:int(m*train_fraction)]\n",
    "test_index = indices_permutados[int(m*train_fraction):]\n",
    "\n",
    "color = ['blue' if y == 1 else 'red' for y in np.squeeze(Y)] # una lista para darle color a las clases\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(X[:,0], X[:,1], color=color)\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilicemos las capas tipo __Dense__ con activaciones __elu__ en las capas intermedias, y __softmax__ para la salida. También utilicemos la capa __Flatten__ para aplanar la imagen de entrada a un vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session() #borra el grafo de la sesion. Util cuando creamos muchos modelos en una sesion.\n",
    "\n",
    "# Red neuronal\n",
    "model = Sequential()\n",
    "model.add(Dense(units=10, use_bias=True, activation='elu', input_dim=2))\n",
    "model.add(Dense(units=1, activation='sigmoid', use_bias=True))\n",
    "\n",
    "opt = RMSprop(lr=0.1)\n",
    "\n",
    "model.compile(optimizer=opt,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutaremos el ciclo de entrenamiento con tres callbacks de la siguiente forma:\n",
    "\n",
    "- __EarlyStopping (callbk1)__: Haremos un monitoreo sobre el error de validación y si crece en muchas ocasiones, detenemos el entrenamiento.\n",
    "- __LearningRateScheduler (callbk2)__: Cada 100 epocas actualizamos el learning rate al 90% del valor actual.\n",
    "- __CustomCallback (callbk3)__: Cada 20 epocas guardaremos una imagen de la clasificación del modelo que luego podemos utilizar para visualizar la dinamica del entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler, Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "\n",
    "def schedule(epoch, lr):\n",
    "    \n",
    "    # Cada 100 epocas actualice el learning rate al 0.9 del actual\n",
    "    \n",
    "    if((epoch+1) % 100 == 0):\n",
    "        lr = lr * 0.9\n",
    "        print(\"Learning rate update: %.4f \\n\"%(lr))\n",
    "\n",
    "    return lr\n",
    "\n",
    "class CustomCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if ((epoch) % 20 == 0):\n",
    "            visualize_model(model, X, Y, output='same', \n",
    "                            save=True, \n",
    "                            save_path='img'+str(epoch+1)+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbk1 = EarlyStopping(monitor='val_loss', \n",
    "                       mode='min',\n",
    "                       patience=50,\n",
    "                       restore_best_weights=True,\n",
    "                       verbose=True)\n",
    "\n",
    "callbk2 = LearningRateScheduler(schedule)\n",
    "\n",
    "callbk3 = CustomCallback()\n",
    "\n",
    "history = model.fit(X[train_index], Y[train_index],\n",
    "                    validation_data=[X[test_index], Y[test_index]],\n",
    "                    epochs=1000, verbose=2,\n",
    "                    callbacks=[callbk1, callbk2, callbk3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Error (Loss) final: %.4E\"%(np.array(history.history['loss'][-1:]))) #Error final de la lista de errores\n",
    "print(\"Precision (Accuracy) final: %.4f\"%(np.array(history.history['acc'][-1:])))\n",
    "\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.plot(history.history['acc'])\n",
    "\n",
    "plt.legend(['train_loss', 'val_loss', 'train_acc', 'val_acc'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_model(model, X, Y, output='truncate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trabajemos\n",
    "\n",
    "Cree un Callback para que termine el entrenamiento si la diferencia entre el error de entrenamiento y el de validación supera un umbral dado.\n",
    "\n",
    "Para ello debe utilizar, el diccionario __logs__ dentro de la función __on_epoch_end__ y __self.model.stop_training = True__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<hr>\n",
    "\n",
    "## 2. Lambda layers\n",
    "\n",
    "Las capas lambda permiten crear capas personalizadas que ejecuten una función Lambda. Por ejemplo, promediar los valores de una capa anterior, realizar una operación aritmética sobre una capa, concatenar la activación de dos o más capas, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funciones / expresiones Lambda\n",
    "\n",
    "f = lambda x: x**0.5\n",
    "g = lambda a,b: a**2 + 2*a*b + b**2\n",
    "\n",
    "print(f(2))\n",
    "print(g(2,4))\n",
    "\n",
    "fact = lambda x: 1 if x == 0 else x * fact(x-1)\n",
    "print(\"Factorial de %i: %i\"%(4,fact(4)))\n",
    "\n",
    "fib = lambda i: i if i < 2 else fib(i-1)+fib(i-2)\n",
    "print(\"%i-esimo elemento Fibonnaci: %i\"%(20, fib(20)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora crearemos un modelo con una capa Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session() #borra el grafo de la sesion. Util cuando creamos muchos modelos en una sesion.\n",
    "\n",
    "# Red neuronal\n",
    "model = Sequential()\n",
    "model.add(Dense(units=10, use_bias=True, activation='linear', input_dim=2))\n",
    "\n",
    "model.add(Lambda(lambda x: (K.exp(x) - K.exp(-x)) / (K.exp(x) + K.exp(-x)), \n",
    "                 name='tanh'))\n",
    "\n",
    "model.add(Dense(units=1, activation='sigmoid', use_bias=True))\n",
    "\n",
    "opt = RMSprop(lr=0.1)\n",
    "\n",
    "model.compile(optimizer=opt,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X[train_index], Y[train_index],\n",
    "                    validation_data=[X[test_index], Y[test_index]],\n",
    "                    epochs=300, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_model(model, X, Y, output='truncate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "x = np.linspace(-10,10,50)\n",
    "plt.plot(x,relu(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Trabajemos\n",
    "\n",
    "Utilizando como base la función relu implementada sobre numpy, implemente una capa Lambda que ejecute la función relu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Las capas Lambda no se limitan a trabajar con expresiones Lambda. Podemos hacer un llamado a una función externa y utilizar el cableado tipo C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layers_sum(vects):\n",
    "    x, y = vects\n",
    "    return x+y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session() #borra el grafo de la sesion. Util cuando creamos muchos modelos en una sesion.\n",
    "\n",
    "x1 = Input(shape=(1,))\n",
    "x2 = Input(shape=(2,))\n",
    "\n",
    "a1 = Dense(units=10, activation='relu')(x1)\n",
    "a1 = Dense(units=10, activation='relu')(a1)\n",
    "\n",
    "a2 = Dense(units=10, activation='relu')(x2)\n",
    "\n",
    "a3 = Lambda(layers_sum, output_shape=(10,))([a1,a2])\n",
    "output = Dense(units=1, activation='sigmoid')(a3)\n",
    "\n",
    "model = Model(inputs=[x1,x2], outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 3. Estimación del learning rate $\\alpha$\n",
    "\n",
    "Una forma de estimar este meta-parámetro, consiste en entrenar el modelo haciendo un barrido de $\\alpha$ mediante CallBacks, plotear el comportamiento del error, y seleccionar el valor límite de $\\alpha$ donde el error se mantiene estable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session() #borra el grafo de la sesion. Util cuando creamos muchos modelos en una sesion.\n",
    "\n",
    "initial_lr = 1e-4\n",
    "\n",
    "# Red neuronal\n",
    "model = Sequential()\n",
    "model.add(Dense(units=10, use_bias=True, activation='elu', input_dim=2))\n",
    "model.add(Dense(units=1, activation='sigmoid', use_bias=True))\n",
    "\n",
    "opt = RMSprop(lr=initial_lr)\n",
    "\n",
    "model.compile(optimizer=opt,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 75\n",
    "\n",
    "lr_schedule = LearningRateScheduler(lambda epoch: initial_lr * 10**(epoch / 20))\n",
    "\n",
    "history = model.fit(X[train_index], Y[train_index], \n",
    "                    epochs=num_epochs, \n",
    "                    callbacks=[lr_schedule], verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lr = initial_lr * 10**(num_epochs / 20)\n",
    "\n",
    "plt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\n",
    "plt.axis([initial_lr, max_lr, 0, np.max(history.history[\"loss\"])])\n",
    "\n",
    "plt.xlabel(\"learning rate\")\n",
    "plt.ylabel(\"loss\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session() #borra el grafo de la sesion. Util cuando creamos muchos modelos en una sesion.\n",
    "\n",
    "lr = 3e-2\n",
    "\n",
    "# Red neuronal\n",
    "model = Sequential()\n",
    "model.add(Dense(units=10, use_bias=True, activation='elu', input_dim=2))\n",
    "model.add(Dense(units=1, activation='sigmoid', use_bias=True))\n",
    "\n",
    "opt = RMSprop(lr=lr)\n",
    "\n",
    "model.compile(optimizer=opt,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 500\n",
    "\n",
    "history = model.fit(X[train_index], Y[train_index], \n",
    "                    validation_data=[X[test_index], Y[test_index]],\n",
    "                    epochs=num_epochs, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_model(model, X, Y, output='same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Error (Loss) final: %.4E\"%(np.array(history.history['loss'][-1:]))) #Error final de la lista de errores\n",
    "print(\"Precision (Accuracy) final: %.4f\"%(np.array(history.history['acc'][-1:])))\n",
    "\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.plot(history.history['acc'])\n",
    "\n",
    "plt.legend(['train_loss', 'val_loss', 'train_acc', 'val_acc'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 4. Grid Search\n",
    "\n",
    "Grid-search consiste en proceso de búsqueda de meta-parámetros asociados a un modelo de predicción. Para esto utilizaremos un wrapper de Keras$\\rightarrow$scikit-learn con el objetivo de utilizar el grid-search de sklearn sobre un modelo de Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos el wrapper de Keras->sklearn\n",
    "\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Primero creamos una funcion que crea la red neuronal y--\n",
    "#--pasamos a esta funcion los parametros asociados al modelo que desean ser explorados.\n",
    "\n",
    "def crear_modelo(lr=3e-2, n_units=2):\n",
    "    \n",
    "    # Red neuronal\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=n_units, use_bias=True, activation='elu', input_dim=2))\n",
    "    model.add(Dense(units=1, activation='sigmoid', use_bias=True))\n",
    "\n",
    "    opt = RMSprop(lr=lr)\n",
    "\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ahora creamos un modelo KerasClassifier.\n",
    "#El constructor de este wrapper puede acceder a los parametros--\n",
    "#--de la funcion de creacion y a los de model.fit().\n",
    "\n",
    "keras_model = KerasClassifier(build_fn=crear_modelo, n_units=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos el grid-search de sklearn\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creamos el diccionario de parametros a ser explorados\n",
    "\n",
    "param_grid = dict(n_units=[5,10,15], \n",
    "                  epochs=[350,400], \n",
    "                  batch_size=[8,16])\n",
    "\n",
    "#cv=cross-validation\n",
    "grid = GridSearchCV(estimator=keras_model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimir resultado\n",
    "\n",
    "print(\"Mejor acc(val. cruz.): %.3f utilizando %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%.3f (%.4f) con: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 5. Custom losses (errores personalizados)\n",
    "\n",
    "En algunas ocasiones es posible necesitar funciones de error personalizadas las cuales no pueden construidas directamente a partir de las funciones de error de Keras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos una funcion que recibe los parametros __y_true__ y __y_pred__.\n",
    "\n",
    "def error_func(y_true, y_pred):\n",
    "    '''\n",
    "    Log Loss\n",
    "    '''\n",
    "    loss = -(y_true*K.log(y_pred) + (K.constant(1.0)-y_true)*K.log(K.constant(1.0)-y_pred))\n",
    "    \n",
    "    cost = K.mean(loss)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session() #borra el grafo de la sesion. Util cuando creamos muchos modelos en una sesion.\n",
    "\n",
    "lr = 3e-2\n",
    "\n",
    "# Red neuronal\n",
    "model = Sequential()\n",
    "model.add(Dense(units=10, use_bias=True, activation='elu', input_dim=2))\n",
    "model.add(Dense(units=1, activation='sigmoid', use_bias=True))\n",
    "\n",
    "opt = RMSprop(lr=lr)\n",
    "\n",
    "model.compile(optimizer=opt,\n",
    "              loss=error_func, #utilizamos nuestra funcion de error\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X[train_index], Y[train_index],\n",
    "                    validation_data=[X[test_index], Y[test_index]],\n",
    "                    epochs=300, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_model(model, X, Y, output='same')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Analicemos\n",
    "\n",
    "Debido a la restriccion en Keras de que las loss functions reciben exclusivamente los parametros y_pred y y_true, en caso de necesitar funciones de error que reciben distintos o mayor numero de parametros, podemos utilizar funcion clojure:\n",
    "<br>\n",
    "https://towardsdatascience.com/advanced-keras-constructing-complex-custom-losses-and-metrics-c07ca130a618"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ejemplo de clojure\n",
    "\n",
    "def funcion1(c):\n",
    "    \n",
    "    def funcion2(a,b):\n",
    "        return(a+b+c)\n",
    "    \n",
    "    return funcion2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = funcion1(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 6. Custom Activations\n",
    "\n",
    "En algunas ocasiones es posible necesitar funciones de activación personalizadas las cuales no se encuentran implementadas en el motor de Keras. \n",
    "\n",
    "Utilicemos como ejemplo la implementación de la función Mish entregada por los autores.\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "<font size=3>\n",
    "    $a = z \\cdot tanh(\\zeta (z))$\n",
    "    <br>\n",
    "    where\n",
    "    <br>\n",
    "    $\\zeta(z) = ln(1+e^z)$\n",
    "</font>\n",
    "</center>\n",
    "\n",
    "https://arxiv.org/pdf/1908.08681.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.linspace(-5,5,50)\n",
    "a = z * np.tanh(np.log(1+np.exp(z)))\n",
    "\n",
    "plt.plot(z,a)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import get_custom_objects\n",
    "\n",
    "class Mish(Activation):\n",
    "    '''\n",
    "    Mish Activation Function.\n",
    "    .. math::\n",
    "        mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + e^{x}))\n",
    "    Shape:\n",
    "        - Input: Arbitrary. Use the keyword argument `input_shape`\n",
    "        (tuple of integers, does not include the samples axis)\n",
    "        when using this layer as the first layer in a model.\n",
    "        - Output: Same shape as the input.\n",
    "    Examples:\n",
    "        >>> X = Activation('Mish', name=\"conv1_act\")(X_input)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, activation, **kwargs):\n",
    "        super(Mish, self).__init__(activation, **kwargs)\n",
    "        self.__name__ = 'Mish'\n",
    "\n",
    "\n",
    "def mish(inputs):\n",
    "    #return inputs * tf.math.tanh(tf.math.softplus(inputs))\n",
    "    return inputs * tf.math.tanh(tf.log(1+tf.exp(inputs)))\n",
    "\n",
    "#actualizamos el diccionario global del objetos propios\n",
    "get_custom_objects().update({'Mish': Mish(mish)}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session() #borra el grafo de la sesion. Util cuando creamos muchos modelos en una sesion.\n",
    "\n",
    "lr = 3e-2\n",
    "\n",
    "# Red neuronal\n",
    "model = Sequential()\n",
    "model.add(Dense(units=10, use_bias=True, input_dim=2))\n",
    "model.add(Activation(activation='Mish'))\n",
    "model.add(Dense(units=1, activation='sigmoid', use_bias=True))\n",
    "\n",
    "opt = RMSprop(lr=lr)\n",
    "\n",
    "model.compile(optimizer=opt,\n",
    "              loss='binary_crossentropy', #utilizamos nuestra funcion de error\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X[train_index], Y[train_index],\n",
    "                    validation_data=[X[test_index], Y[test_index]],\n",
    "                    epochs=500, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_model(model, X[test_index], Y[test_index], output='truncate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
